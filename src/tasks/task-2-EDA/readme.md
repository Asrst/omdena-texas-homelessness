# Task-2-EDA
## Resources
* app.py is designed for you to be able to do base-line data preprocessing and cleaning on the datasets provided through Omdena. Please let me know if you have any other suggestions on what we should do to add to this file as the collaborative effort on this is key to our success. If we can get this functioning at a high enough level, we can turn this over to the modeling team and the app building team as it will help them in the long run!

* Notebook Folder
  * This folder holds my initial exploratory notebooks with some key visuals to help you get started on a few different questions as you work to develop your own. 
  * These notebooks were built using some of the datasets already located within our Google Drive Doc as well as the Data Folder of this Git Repo. 
  * I hope you can build upon these visuals as well as you progress through your data exploration.
  
* Team Notebook Folders
  * If you participated in this activity with more than yourself, please place your notebooks here. We want to see those who are working with others and making that effort to colaborate. You can put into the Readme.md the colaborators names and link the notebook to it!
  
* Projects Tab
  * Has tasks in the form of "Notes" that you can grab, drag, and drop in different lists, as well as iterate over with your work completed
  * This is a great way for us to also keep track of contributions!  

## What is EDA? 
Check [this link here](https://www.ibm.com/cloud/learn/exploratory-data-analysis) for some more information on exactly what we mean when we say EDA. We are focused on analyzing the data, summarizing the main features, and use visualizations to show the data. Feature Engineering, Statistical Modeling, Hypothesis Testing, and Dimensionality Reduction. These all fall under our category. We will need to understand the data and how everything correlates in order to effectively accomplish this task. 

## Task & Task List
Our task is to complete the EDA so that our Modeling team can intake the data and run it into their models and focus on hyperparameter tuning. We will complete this by splitting the work flow as follows:

* Data Visualization and Normalizing
* Feature Engineering and Dimension Reduction
 * Target Identification and Feature Identification. What works with the data, what doesn't?
  * Do we need to create a target? How can we given the data provided?
  * If provided a target, are we sure that there isn't any columns in the data associated with the calculation of that target that can lead to Data Leakage?
  * What features are present in the data? Given these features, can we combine into one that may hold more significance than the two seperately?
  * 

* Insights and DataSet Summarizations
 * Just as I posed questions to you guys at the beginning of the project in my EDA notebooks, what insights has the data given you into homelessness? Can we incur any hypothesis? Can this be tested? 
 * When we have the Data all pieced together, what does that look like? What is included in each of these data sets? The more information we can pass along to the next team about our data, the better

* Statistical Model Testing of Data
 * Ensure data fits into a model and can attain higher than baseline accuracy.
 * Once that is achieved, I would consider that a "win" that we can pass along to Task 3 for deployment modeling
 * Anyone who is in Task 3 as well is encouraged to take on this task!
